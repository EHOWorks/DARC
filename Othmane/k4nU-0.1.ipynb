{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "import darc_core\n",
    "import sys\n",
    "import hashlib as hs\n",
    "import base64 as b64\n",
    "import os\n",
    "import random as rd\n",
    "import datetime as dt\n",
    "#from darc_core.metrics import Metrics\n",
    "from darc_core.utils import M_COL, T_COL, T_COL_IT, NB_GUESS, SIZE_POOL\n",
    "from darc_core.preprocessing import round1_preprocessing\n",
    "from darc_core.utils import check_format_trans_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 228 ms, total: 23.4 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"./ground_truth.csv\", parse_dates=[\"date\"])\n",
    "df[\"hours\"]= pd.to_datetime(df[\"hours\"], format = '%H:%M%S').dt.hour\n",
    "df[\"month\"]=df[\"date\"].dt.month\n",
    "df[\"year\"]=df[\"date\"].dt.year\n",
    "df[\"id_user\"]=df[\"id_user\"].astype(str)\n",
    "df[\"price\"]=df[\"price\"].astype(\"float\")\n",
    "df[\"qty\"]=df[\"qty\"].astype(\"float\")\n",
    "df_dict = df.T.to_dict('list')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_user</th>\n",
       "      <th>date</th>\n",
       "      <th>hours</th>\n",
       "      <th>id_item</th>\n",
       "      <th>price</th>\n",
       "      <th>qty</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17850</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>8</td>\n",
       "      <td>85123A</td>\n",
       "      <td>2.55</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17850</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>8</td>\n",
       "      <td>71053</td>\n",
       "      <td>3.39</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17850</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>8</td>\n",
       "      <td>84406B</td>\n",
       "      <td>2.75</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17850</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>8</td>\n",
       "      <td>84029G</td>\n",
       "      <td>3.39</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17850</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>8</td>\n",
       "      <td>84029E</td>\n",
       "      <td>3.39</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_user       date  hours id_item  price  qty  month  year\n",
       "0   17850 2010-12-01      8  85123A   2.55  6.0     12  2010\n",
       "1   17850 2010-12-01      8   71053   3.39  6.0     12  2010\n",
       "2   17850 2010-12-01      8  84406B   2.75  8.0     12  2010\n",
       "3   17850 2010-12-01      8  84029G   3.39  6.0     12  2010\n",
       "4   17850 2010-12-01      8  84029E   3.39  6.0     12  2010"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 25s, sys: 4.8 s, total: 12min 30s\n",
      "Wall time: 12min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def mergeIdentical(df,df_dict):\n",
    "    \"\"\" Merge \"same\" transaction into one line. Assign \"DEL\" to duplicates\n",
    "        Transactions are \"identical\" if their keys are equal\n",
    "        \n",
    "        :map: key:[id_user,date,id_item] value:[qty_cumulative,count]\n",
    "    \"\"\"\n",
    "    map={}\n",
    "    for d in range(0,len(df)):\n",
    "        if(map.get((df[\"id_user\"][d],df[\"date\"][d],df[\"id_item\"][d]))):\n",
    "            map[(df[\"id_user\"][d],df[\"date\"][d],df[\"id_item\"][d])][0] += df_dict[d][5]\n",
    "            map[(df[\"id_user\"][d],df[\"date\"][d],df[\"id_item\"][d])][1] += 1\n",
    "            df_dict[d][0]=\"DEL\"\n",
    "            df_dict[d][1]=df_dict[d][1].strftime(\"%Y-%m-01\")\n",
    "            df_dict[d][2]=\"\"\n",
    "            df_dict[d][3]=\"\"\n",
    "            df_dict[d][4]=\"\"\n",
    "            df_dict[d][5]=\"\"\n",
    "        else:\n",
    "            map[(df[\"id_user\"][d],df[\"date\"][d],df[\"id_item\"][d])]=[df.loc[d][\"qty\"],1]\n",
    "    \n",
    "    dfn = pd.DataFrame.from_dict(df_dict,orient=\"index\")\n",
    "    dfn = dfn.rename(columns={0:\"id_user\",1:\"date\",2:\"hours\",3:\"id_item\",4:\"price\",5:\"qty\",6:\"month\",7:\"year\"})\n",
    "    \n",
    "    for i in map:\n",
    "        if(map[i][1]>1):\n",
    "            #map[i][0]=map[i][0]/map[i][1]\n",
    "            mask= (df[\"id_user\"]==i[0]) & (df[\"date\"]==i[1]) & (df[\"id_item\"]==i[2])\n",
    "            dfn.loc[mask,[\"qty\"]]=map[i][0]\n",
    "    \n",
    "  \n",
    "    return dfn,map\n",
    "\n",
    "dfn,map=mergeIdentical(df,df_dict)\n",
    "save_dfn_merge=dfn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, partition, column): \n",
    "    \"\"\"\n",
    "        Split df[partition] over column on median \n",
    "        :return: List[[IndexRange]]*2 {left ,right}\n",
    "    \"\"\"\n",
    "    dfp = df[\"id_user\"][partition]\n",
    "    sanitizedPartition = dfp.index[dfp != \"DEL\"]\n",
    "    dfp = df[column][sanitizedPartition]\n",
    "    if (column in categorical):\n",
    "        values = dfp.unique()\n",
    "        dfp.sort_values(inplace=True)\n",
    "        lv=set(values[:len(values)//2])\n",
    "        rv=set(values[len(values)//2:])\n",
    "        return dfp.index[dfp.isin(lv)],dfp.index[dfp.isin(rv)]\n",
    "    else:\n",
    "        median=dfp.median()\n",
    "        dfp.sort_values(inplace=True)\n",
    "        dfl=dfp.index[dfp<median]\n",
    "        dfr=dfp.index[dfp>=median]\n",
    "        return(dfl,dfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_k_anonymous (df, partition, k=1):\n",
    "    if ((len(partition)<k)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 46s, sys: 9.42 s, total: 13min 55s\n",
      "Wall time: 14min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfn[\"id_item\"]=dfn[\"id_item\"].astype(\"category\")\n",
    "categorical = set ({'id_item'})\n",
    "def partition_dataset(df, is_valid):\n",
    "    \"\"\"\n",
    "        Greedy search\n",
    "        Split dataframe over {year,month,id_item} \n",
    "        :return: List[[IndexRange]] \n",
    "    \"\"\"\n",
    "    finished_partitions=[]\n",
    "    partitions=[df.index]\n",
    "    while partitions:    \n",
    "        partition = partitions.pop(0)\n",
    "        columns = [\"year\",\"month\", \"id_item\"]\n",
    "        for column in columns:\n",
    "            lp, rp = split(df,partition,column)\n",
    "            if not is_valid(df,lp) or not is_valid(df,rp):\n",
    "                continue\n",
    "            partitions.extend((lp,rp))\n",
    "            break\n",
    "        else:\n",
    "            finished_partitions.append(partition)\n",
    "    return finished_partitions\n",
    "finished_partitions = partition_dataset(dfn, is_k_anonymous)\n",
    "save_finished_partitions=finished_partitions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 29s, sys: 18.3 s, total: 13min 47s\n",
      "Wall time: 14min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfn[\"id_item\"]=dfn[\"id_item\"].astype(\"category\")\n",
    "categorical = set ({'id_item'})\n",
    "finished_partitions=save_finished_partitions\n",
    "def make_l_diverse(df,partitions,l=3):\n",
    "    \"\"\"\n",
    "         DEL partitions with l-diversity < l\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    filtered=[]\n",
    "    while(len(partitions)):\n",
    "        if (df.loc[partitions[0],\"id_user\"].nunique()<l):\n",
    "            df.loc[partitions[0],\"id_user\"]=\"DEL\"\n",
    "            if(isinstance(df.loc[partitions[0],[\"date\"]].squeeze(),dt.date)):\n",
    "                df.loc[partitions[0],[\"date\"]]=df.loc[partitions[0],[\"date\"]].squeeze().strftime('%Y-%m-01')\n",
    "            else:\n",
    "                df.loc[partitions[0],[\"date\"]]=df.loc[partitions[0],[\"date\"]].squeeze().apply(lambda x: x.strftime('%Y-%m-01'))\n",
    "            df.loc[partitions[0],[\"hours\",\"id_item\"]]=\"\" \n",
    "            partitions.pop(0)           \n",
    "        else:\n",
    "            filtered.append(partitions.pop(0))       \n",
    "    return dfn, filtered\n",
    "dfn, finished_partitions = make_l_diverse(dfn,finished_partitions)\n",
    "save_diversity=finished_partitions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleDates(df,partition):\n",
    "    \"\"\"\n",
    "         For same partition, swap {dates,qty} randomly.\n",
    "    \"\"\"\n",
    "    date_array=df.loc[partition,\"date\"].copy().squeeze()\n",
    "    qty_array=df.loc[partition,\"qty\"].copy().squeeze()\n",
    "    date_array.reset_index(drop=True,inplace=True) \n",
    "    qty_array.reset_index(drop=True,inplace=True)\n",
    "    date_shuffled=[]\n",
    "    qty_shuffled=[]\n",
    "    while(len(date_array)):\n",
    "        i=rd.randrange(len(date_array))\n",
    "        date_shuffled.append(date_array[i])\n",
    "        date_array.drop(i,inplace=True)\n",
    "        date_array.reset_index(drop=True,inplace=True)\n",
    "        qty_shuffled.append(qty_array[i])\n",
    "        qty_array.drop(i,inplace=True)\n",
    "        qty_array.reset_index(drop=True,inplace=True)\n",
    "    df.loc[partition,\"date\"]=date_shuffled\n",
    "    df.loc[partition,\"qty\"]=qty_shuffled\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 45s, sys: 37.6 s, total: 25min 23s\n",
      "Wall time: 26min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_anonymized_dataset(df,partitions):\n",
    "    \"\"\"\n",
    "        Generalization of Columns {dates,qty,hours}\n",
    "    \"\"\"\n",
    "    for p in partitions:\n",
    "        df.loc[p,[\"hours\"]]=df.loc[p,[\"hours\"]].mean().values[0]\n",
    "        df=shuffleDates(df,p)\n",
    "    return df\n",
    "dfn=build_anonymized_dataset(dfn,finished_partitions)\n",
    "save_dfn=dfn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.88 s, sys: 632 ms, total: 7.52 s\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfn.id_user=dfn.id_user.astype(str)\n",
    "dfn_copy=dfn.copy()\n",
    "\n",
    "def generateSalts():\n",
    "    \"\"\" \n",
    "        Generate Salt{256b} Table : Mij = Salt for user i in month j \n",
    "    \"\"\"\n",
    "    # Set of {1..12} (number of the month)\n",
    "    ordered_months = list(range(1,13))\n",
    "    # Set of all unique user's IDs\n",
    "    ids = dfn_copy[\"id_user\"].unique()\n",
    "    ids = ids[ids != \"DEL\"]\n",
    "    # Generate salt table\n",
    "    salt_table=pd.DataFrame(columns=ordered_months,index=ids)\n",
    "    salt_table.set_index(ids,inplace=True)\n",
    "    for i in ids:\n",
    "        for j in ordered_months:\n",
    "            # Generates unique b64 values\n",
    "            salt_table[j][i]=os.urandom(256)\n",
    "    return salt_table\n",
    "\n",
    "salt_table = generateSalts()\n",
    "salt_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashme(id_user, month): \n",
    "    \"\"\"\n",
    "        :return: Hash[user,month] = SHA256(Salt[user,month] + id_user)\n",
    "    \"\"\"\n",
    "    if(id_user==\"DEL\"):\n",
    "        return id_user\n",
    "    else:\n",
    "        hashme.counter+=1\n",
    "        percent=(hashme.counter/307054)*100 \n",
    "        sys.stdout.write(\"\\rProgress %i -- Count : %i / 307054\" % (percent,hashme.counter))\n",
    "        sys.stdout.flush()\n",
    "        return hs.sha256(salt_table.loc[[id_user],month][0] + id_user.encode()).hexdigest()\n",
    "#Static Counter for ProgressBar\n",
    "hashme.counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 93 -- Count : 285896 / 307054CPU times: user 16min 40s, sys: 2min, total: 18min 40s\n",
      "Wall time: 23min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def doHash():\n",
    "    \"\"\"\n",
    "        Apply hashme for all rows\n",
    "    \"\"\"\n",
    "    dfn_copy.sort_values([\"id_user\",\"month\"])\n",
    "    salt_table.sort_index(inplace=True)\n",
    "    dfn[\"id_user\"]=dfn_copy.apply(lambda x: hashme(x[\"id_user\"],x[\"month\"]), axis=1)\n",
    "doHash()\n",
    "hash_dfn=dfn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 s, sys: 414 ms, total: 31.6 s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def hour_to_int(hours):\n",
    "    if(isinstance(hours,float)):\n",
    "        return int(hours)\n",
    "def parse_dates(date):\n",
    "    if(isinstance(date,dt.date) and not pd.isnull(date)):\n",
    "        return date.strftime('%Y/%m/%d')\n",
    "dfn[\"qty\"]=df[\"qty\"].astype(\"int\")\n",
    "dfn[\"price\"]=df[\"price\"].round(2)\n",
    "dfn[\"date\"]=dfn[\"date\"].apply(lambda x: x.strftime('%Y/%m/%d'))\n",
    "dfn[\"hours\"]=dfn.apply(lambda x: hour_to_int(x[\"hours\"]), axis=1)\n",
    "dfn.drop([\"month\",\"year\"],axis=1,inplace=True)\n",
    "dfn.sort_values([\"date\"],inplace=True)\n",
    "dfn.to_csv(\"./atxtest.csv\",index=False)\n",
    "dfa=pd.read_csv(\"./atxtest.csv\")\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 389 ms, total: 14.4 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ground_truth, submission = round1_preprocessing(\n",
    "    \"./ground_truth.csv\", \"./atx5.csv\"\n",
    ")\n",
    "check_format_trans_file(ground_truth, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:238: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:238: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:238: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:238: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:238: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:238: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:238: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 16s, sys: 27.5 s, total: 30min 43s\n",
      "Wall time: 1h 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metric = Metrics(ground_truth, submission)\n",
    "scores = metric.scores_reid()\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001156, 0.000267, 0.000178, 0.002046, 0.000178, 0.002224, 0.003439]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8568029966868055,\n",
       " 0,\n",
       " 0.7850147308995272,\n",
       " 0.0,\n",
       " 0.431553882,\n",
       " 0.2057,\n",
       " 0.028296,\n",
       " 0.000177,\n",
       " 8.8e-05,\n",
       " 0.014767,\n",
       " 8.8e-05,\n",
       " 0.01291,\n",
       " 0.00336]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = metric.scores_util()\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn=pd.read_csv(\"./atx1.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 1         24\n",
       "2         24\n",
       "4         24\n",
       "15         4\n",
       "19         2\n",
       "27        12\n",
       "29        48\n",
       "31        48\n",
       "33        12\n",
       "43        12\n",
       "44        12\n",
       "50        12\n",
       "51         1\n",
       "53         1\n",
       "57         1\n",
       "58         1\n",
       "59         2\n",
       "60         2\n",
       "63        12\n",
       "64         6\n",
       "67         2\n",
       "70         1\n",
       "76        25\n",
       "77        25\n",
       "83        25\n",
       "85        24\n",
       "92         8\n",
       "94         1\n",
       "96         1\n",
       "97         1\n",
       "          ..\n",
       "306763    25\n",
       "306812    12\n",
       "306813    12\n",
       "306815     6\n",
       "306829     2\n",
       "306836     4\n",
       "306846    12\n",
       "306861    12\n",
       "306864    12\n",
       "306867    24\n",
       "306920    12\n",
       "306924    24\n",
       "306947    48\n",
       "306949    12\n",
       "306950     4\n",
       "306951    12\n",
       "306953    12\n",
       "306955    12\n",
       "306957    96\n",
       "306971     8\n",
       "306972    12\n",
       "306973    24\n",
       "306974    12\n",
       "306976    12\n",
       "306993    15\n",
       "306994    15\n",
       "307026    20\n",
       "307035    12\n",
       "307043    12\n",
       "307049    12\n",
       "Name: qty, Length: 63166, dtype: int64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn[dfn[\"id_user\"]==\"DEL\"][\"qty\"].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=dfn[\"id_user\"]==\"DEL\"\n",
    "dfn.loc[mask,\"date\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.loc[mask,[\"qty\",\"price\",\"hours\",\"id_item\"]]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(date):\n",
    "    if(isinstance(date,dt.date)):\n",
    "        return date.strftime('%Y/%m/%d')\n",
    "dfn[\"date\"]=dfn.apply(lambda x: parse_dates(x[\"date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.to_csv(\"./atx2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn=hash_dfn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn[\"date\"]=dfn[\"date\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011-11-01    3017\n",
       "2011-10-01    2455\n",
       "2010-12-01    2154\n",
       "2011-09-01    1532\n",
       "2011-05-01    1466\n",
       "2011-03-01    1441\n",
       "2011-06-01    1437\n",
       "2011-08-01    1412\n",
       "2011-01-01    1384\n",
       "2011-04-01    1359\n",
       "2011-02-01    1357\n",
       "2011-07-01    1352\n",
       "2011-12-01     792\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn[dfn[\"id_user\"]==\"DEL\"][\"date\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2010-12-16\n",
       "1        2010-12-07\n",
       "2        2010-12-19\n",
       "3        2010-12-07\n",
       "4        2010-12-08\n",
       "5        2010-12-01\n",
       "6        2010-12-06\n",
       "7        2010-12-05\n",
       "8        2010-12-06\n",
       "9        2010-12-06\n",
       "10       2010-12-02\n",
       "11       2010-12-14\n",
       "12       2010-12-08\n",
       "13       2010-12-06\n",
       "14       2010-12-03\n",
       "15       2010-12-10\n",
       "16       2010-12-16\n",
       "17       2010-12-14\n",
       "18       2010-12-01\n",
       "19       2010-12-01\n",
       "20       2010-12-14\n",
       "21       2010-12-01\n",
       "22       2010-12-05\n",
       "23       2010-12-15\n",
       "24       2010-12-13\n",
       "25       2010-12-21\n",
       "26       2010-12-19\n",
       "27       2010-12-05\n",
       "28       2010-12-09\n",
       "29       2010-12-01\n",
       "            ...    \n",
       "307024   2011-12-08\n",
       "307025   2011-12-06\n",
       "307026   2011-12-09\n",
       "307027   2011-11-13\n",
       "307028   2011-11-17\n",
       "307029   2011-11-14\n",
       "307030   2011-11-18\n",
       "307031   2011-12-02\n",
       "307032   2011-11-20\n",
       "307033   2011-11-13\n",
       "307034   2011-11-28\n",
       "307035   2011-12-05\n",
       "307036   2011-11-23\n",
       "307037   2011-11-11\n",
       "307038   2011-11-30\n",
       "307039   2011-11-21\n",
       "307040   2011-11-06\n",
       "307041   2011-11-29\n",
       "307042   2011-11-21\n",
       "307043   2011-12-01\n",
       "307044   2011-12-01\n",
       "307045   2011-11-29\n",
       "307046   2011-12-06\n",
       "307047   2011-11-27\n",
       "307048   2011-11-04\n",
       "307049   2011-11-13\n",
       "307050   2011-11-11\n",
       "307051   2011-11-23\n",
       "307052   2011-11-25\n",
       "307053   2011-11-08\n",
       "Name: date, Length: 307054, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File: metrics.py\n",
    "Author: Antoine Laurent\n",
    "Email: laurent.antoine@courrier.uqam.ca\n",
    "Github: https://github.com/Drayer34\n",
    "Description: class for all re-identification metrics\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "from pathos.multiprocessing import ProcessingPool as PPool\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "class Metrics(object):\n",
    "\n",
    "    \"\"\"Super class Metrics for ReidentificationMetrics and UtilityMetrics. It genreate the S data\n",
    "    from AT one.\n",
    "\n",
    "    Attributes are :\n",
    "        :_users: M table containing all users present in the transaction data T (pandas DataFrame).\n",
    "        :_ground_truth: T table containing all transaction of all user for one year (pandas DataFrame).\n",
    "        :_anonymized: S table, the anonymized version of the _ground_truth (pandas DataFrame).\n",
    "        :_users_t_col: the name of the columns in the csv file M.\n",
    "        :_gt_t_col: the name of the columns in the csv file T.\n",
    "        :_current_score: current score calculated by the metric already processed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T, AT, M_col=M_COL, T_col=T_COL, T_col_it=T_COL_IT):\n",
    "        \"\"\"\n",
    "        :_users: M table containing all users present in the transaction data T (pandas DataFrame).\n",
    "        :_ground_truth: T table containing all transaction of all user for one year (pandas DataFrame).\n",
    "        :_anonymized: S table, the anonymized version of the _ground_truth (pandas DataFrame).\n",
    "        :_users_t_col: the name of the columns in the csv file M.\n",
    "        :_gt_t_col: the name of the columns in the csv file T.\n",
    "        :_gt_t_col: the name of the columns in the csv file T for iteration.\n",
    "        :_current_score: current score calculated by the metric already processed.\n",
    "        \"\"\"\n",
    "        self._ground_truth = T\n",
    "        self._users = pd.DataFrame(self._ground_truth.id_user.drop_duplicates().sort_values(), columns=[\"id_user\"]).reset_index(drop=True)\n",
    "        self._anon_trans = AT\n",
    "        self._users_t_col = M_col\n",
    "        self._gt_t_col = T_col\n",
    "        self._gt_t_col_it = T_col_it\n",
    "        self._current_score = []\n",
    "\n",
    "        #  TODO: Cast all data into a fonction  <07-12-18, yourname> #\n",
    "        self._ground_truth[self._gt_t_col['id_user']] = self._ground_truth[self._gt_t_col['id_user']].apply(str)\n",
    "        self._anon_trans[self._gt_t_col['id_user']] = self._anon_trans[self._gt_t_col['id_user']].apply(str)\n",
    "\n",
    "        self._anonymized = self.generate_S_data()\n",
    "\n",
    "        self._f_orig = self.f_orig()\n",
    "\n",
    "\n",
    "    def scores_util(self):\n",
    "        return [self._e1_metric(), self._e2_metric(), self._e3_metric(), self._e4_metric(), self._e5_metric(), self._e6_metric()]\n",
    "\n",
    "    def scores_reid(self):\n",
    "        return [self._s1_metric(), self._s2_metric(), self._s3_metric(), self._s4_metric(), self._s5_metric(), self._s6_metric(), self._s7_metric()]\n",
    "\n",
    "    def scores(self):\n",
    "        return self.scores_util()\n",
    "\n",
    "    def generate_S_data(self):\n",
    "        \"\"\"Generate S data from AT data.\n",
    "        :returns: S\n",
    "\n",
    "        \"\"\"\n",
    "        data = self._anon_trans.copy()\n",
    "        # Remove NaN value from DataFrame\n",
    "        data = data.dropna()\n",
    "        # Remove 'DEL' row in DataFrame\n",
    "        data = data[data[self._gt_t_col['id_user']] != \"DEL\"]\n",
    "        #  TODO:check si il y a une seed pour l'aléa  <30-05-18, yourname> #\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def month_passed(self, date):\n",
    "        \"\"\" Get the month from a date, month should be between 0 and 11\n",
    "\n",
    "        :date: a date in format YYYY/MM/DD\n",
    "        :return: integer between 0 and 11 \"\"\"\n",
    "        return 0 if date.split('/')[0] == '2010' else int(date.split('/')[1])\n",
    "\n",
    "    def f_orig(self):\n",
    "        \"\"\"Generate the F file for the original data, to compare it with the F^ file.\n",
    "\n",
    "        :returns: F file original\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialization\n",
    "        f_orig = pd.DataFrame(columns=['id_user', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "        f_orig.id_user = self._ground_truth[self._gt_t_col['id_user']].value_counts().index\n",
    "        f_orig = f_orig.sort_values('id_user').reset_index(drop=True)\n",
    "        f_orig.loc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]] = \"DEL\"\n",
    "\n",
    "\n",
    "        seen = set()\n",
    "        for row in self._ground_truth.itertuples():\n",
    "            id_orig = row[self._gt_t_col_it['id_user']]\n",
    "            month = self.month_passed(row[self._gt_t_col_it['date']])\n",
    "            id_ano = self._anon_trans.loc[row[0], self._gt_t_col['id_user']]\n",
    "            item = \"{}-{}-{}\".format(id_orig, month, id_ano)\n",
    "            if item not in seen and id_ano != \"DEL\":\n",
    "                seen.add(item)\n",
    "                f_orig.loc[f_orig.id_user == id_orig, month] = id_ano\n",
    "\n",
    "        return f_orig\n",
    "\n",
    "    def compare_f_files(self, f_hat):\n",
    "        \"\"\"Compare the two F files to compute the difference and thus the score\n",
    "\n",
    "        :f: the original f file to compare (pandas DataFrame)\n",
    "        :f_hat: the guessed f file computed by the metric or adversary (pandas DataFrame)\n",
    "\n",
    "        :returns: score\n",
    "        \"\"\"\n",
    "\n",
    "        #tps1 = time.clock()\n",
    "        map_error = 0\n",
    "        score = 0\n",
    "        count = 0\n",
    "        total = 0\n",
    "\n",
    "        #we want the same list of users\n",
    "        if set(self._f_orig['id_user']).difference(set(f_hat['id_user'])):\n",
    "            map_error = 1\n",
    "\n",
    "        if map_error == 0:\n",
    "            for row in self._f_orig.itertuples():\n",
    "                # Compare each tuple, if they are egual over all month then gain 12 points\n",
    "                # One points per similarities\n",
    "                f_ori_tuple = row[1:]\n",
    "                f_hat_tuple = tuple(f_hat[f_hat['id_user'] == row[1]].iloc[0])\n",
    "                for i in range(1, 13):\n",
    "                    if f_ori_tuple[i] != \"DEL\":\n",
    "                        total += 1\n",
    "                        count += self._compare_row_f_file(f_ori_tuple[i], f_hat_tuple[i])\n",
    "        #            if f_ori_tuple[i] == f_hat_tuple[i] and f_ori_tuple[i] != \"DEL\":\n",
    "        #                count += 1\n",
    "\n",
    "\n",
    "        if map_error == 0:\n",
    "            score += round(float(count)/float(total), 6)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compare_row_f_file(self, row_orig, row_sub):\n",
    "        \"\"\"\n",
    "        Check if row_orig is a substring of row_sub, i.e did the participant submit\n",
    "        the good id among all id submitted\n",
    "        \"\"\"\n",
    "        row_sub = row_sub.split(':')\n",
    "        count = 0\n",
    "        for i in range(len(row_sub)):\n",
    "            if row_orig in row_sub[i]:\n",
    "                count = (NB_GUESS - i)/NB_GUESS\n",
    "        return count\n",
    "\n",
    "    def _gen_value_id_dic(self, attrs):\n",
    "        \"\"\"Generate the dictionaty which associate the value of the attributes attrs in the\n",
    "        DataFrame to an user ID.\n",
    "\n",
    "        :attrs: the list of attibutes to check for creating the value\n",
    "        :return: dictionary of value:id\n",
    "        \"\"\"\n",
    "        value_dic = {}\n",
    "        for row in self._anonymized.itertuples():\n",
    "            # Create the value with all the row[attrs] concat with \":\"\n",
    "            value = ':'.join([str(row[elt]) for elt in attrs])\n",
    "            value_dic[value] = row[self._gt_t_col_it['id_user']]\n",
    "        return value_dic\n",
    "\n",
    "    def _guess_inialisation(self):\n",
    "        \"\"\"Generate a virgin F^ file with DEL on each column for each id\n",
    "\n",
    "        :return: the dictionary of id:pseudos\n",
    "        \"\"\"\n",
    "        guess = OrderedDict()\n",
    "        for row in self._users.itertuples():\n",
    "            # Fill dic[id] with DEL\n",
    "            guess[str(row[self._users_t_col['id_user']])] = ['DEL' for i in range(13)]\n",
    "        return guess\n",
    "\n",
    "    def _tronc_product_id(self, num):\n",
    "        \"\"\" Tronc the product ID to the number num.\n",
    "            Example :\n",
    "                id_prod = ABCDEF and num = 2\n",
    "                result = AB\n",
    "\n",
    "        :num: the number of characters to keep.\n",
    "        \"\"\"\n",
    "        col_id_item = self._gt_t_col['id_item']\n",
    "        self._anonymized.loc[:, col_id_item] = self._anonymized.loc[:, col_id_item]\\\n",
    "                                            .apply(lambda s: s[:min(len(s), num)])\n",
    "        self._ground_truth.loc[:, col_id_item] = self._ground_truth.loc[:, col_id_item]\\\n",
    "                                            .apply(lambda s: s[:min(len(s), num)])\n",
    "\n",
    "    def _evaluate(self, attrs):\n",
    "        \"\"\" Evaluate the similtude between T and S on attributs attrs.\n",
    "\n",
    "        :attrs: attributes to check.\n",
    "        :return: F^ the guess of Pseudo for each user and each month.\n",
    "        \"\"\"\n",
    "\n",
    "        dic_value_anon = self._gen_value_id_dic(attrs)\n",
    "        guess = self._guess_inialisation()\n",
    "        for row in self._ground_truth.itertuples():\n",
    "            #create the concat of the attributes to watch\n",
    "            value = ':'.join([str(row[i]) for i in attrs])\n",
    "            id_user = row[self._gt_t_col_it['id_user']]\n",
    "            if value in dic_value_anon.keys():\n",
    "                #recover month of the transaction\n",
    "                month = self.month_passed(row[self._gt_t_col_it['date']])\n",
    "                #affect pseudo for id_user where value == value\n",
    "                guess[id_user][month] = dic_value_anon[value]\n",
    "\n",
    "        f_hat = pd.DataFrame(guess).transpose()\n",
    "        f_hat = f_hat.reset_index()\n",
    "        f_hat = f_hat.rename(columns={'index':'id_user'})\n",
    "\n",
    "        return f_hat\n",
    "\n",
    "    def _subset(self, month):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        subset1 = time.clock()\n",
    "        # On définit le masque en fonction du mois, beacoup de cas -> le code est dégeu\n",
    "        if month != 0 :\n",
    "            if month < 9:\n",
    "                start = '2011-0'+str(month)+'-01'\n",
    "                end = '2011-0'+str(month+1) +'-01'\n",
    "            if month == 9:\n",
    "                start = '2011-0'+str(month)+'-01'\n",
    "                end = '2011-10-01'\n",
    "            if month == 12:\n",
    "                start = '2011-'+str(month)+'-01'\n",
    "                end = '2012-01-01'\n",
    "            else:\n",
    "                start = '2011-'+str(month)+'-01'\n",
    "                end = '2011-'+str(month+1) +'-01'\n",
    "        else:\n",
    "            start = '2010-12-01'\n",
    "            end = '2010-12-31'\n",
    "        self._ground_truth['date'] = pd.to_datetime(self._ground_truth['date'])\n",
    "        mask1 = (self._ground_truth['date'] >= start) & (self._ground_truth['date'] < end)\n",
    "        self._anonymized['date'] = pd.to_datetime(self._anonymized['date'])\n",
    "        mask2 = (self._anonymized['date'] >= start) & (self._anonymized['date'] < end)\n",
    "\n",
    "        t_month=self._ground_truth.loc[mask1]\n",
    "        a_month=self._anonymized.loc[mask2]\n",
    "\n",
    "        subset2 = time.clock()\n",
    "        return t_month, a_month, month\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_score(a, t):\n",
    "        \"\"\"\n",
    "        TODO : Find a way to put it in a lambda\n",
    "        \"\"\"\n",
    "        return len(a.intersection(t))/len(t)\n",
    "\n",
    "    def _find_k_guess(self, t_month, a_month, month):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        #k_guess1 = time.clock()\n",
    "\n",
    "        #Fabrication du vecteur d'item pour chaque utilisateur\n",
    "        t_month_group=t_month.groupby('id_user')['id_item'].apply(set)\n",
    "        a_month_group=a_month.groupby('id_user')['id_item'].apply(set)\n",
    "\n",
    "        # Début du calculs des coefficients de similiarité.\n",
    "        d_month=dict()\n",
    "        d_month['month'] = month\n",
    "\n",
    "        for id_t in t_month_group.index:\n",
    "            list_guess = list()\n",
    "            guess = str()\n",
    "            inter_a_t = a_month_group.apply( lambda x : self._compute_score(x, t_month_group.loc[id_t]))\n",
    "            top_guess = inter_a_t.nlargest(NB_GUESS)\n",
    "            for index in top_guess.index :\n",
    "                 guess += str(index) + ':'\n",
    "            d_month[id_t] = guess[:-1]\n",
    "        #k_guess2 = time.clock()\n",
    "        return d_month\n",
    "\n",
    "    def _s1_metric(self):\n",
    "        \"\"\"Calculate metric S1, comparing date and quantity buy on each row.\n",
    "        Update the current score value\n",
    "\n",
    "        :returns: the score on this metric (between 0 and 1)\n",
    "\n",
    "        \"\"\"\n",
    "        date_col = self._gt_t_col_it['date']\n",
    "        qty_col = self._gt_t_col_it['qty']\n",
    "\n",
    "        f_hat = self._evaluate([date_col, qty_col])\n",
    "\n",
    "        score = self.compare_f_files(f_hat)\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _s2_metric(self):\n",
    "        \"\"\"Calculate metric S2, comparing date and quantity buy on each row.\n",
    "\n",
    "        :returns: the score on this metric (between 0 and 1)\n",
    "\n",
    "        \"\"\"\n",
    "        id_item_col = self._gt_t_col_it['id_item']\n",
    "        price_col = self._gt_t_col_it['price']\n",
    "\n",
    "        f_hat = self._evaluate([id_item_col, price_col])\n",
    "\n",
    "        score = self.compare_f_files(f_hat)\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _s3_metric(self):\n",
    "        \"\"\"Calculate metric S3, comparing date and quantity buy on each row.\n",
    "\n",
    "        :returns: the score on this metric (between 0 and 1)\n",
    "\n",
    "        \"\"\"\n",
    "        id_item_col = self._gt_t_col_it['id_item']\n",
    "        qty_col = self._gt_t_col_it['qty']\n",
    "\n",
    "        f_hat = self._evaluate([id_item_col, qty_col])\n",
    "\n",
    "        score = self.compare_f_files(f_hat)\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _s4_metric(self):\n",
    "        \"\"\"Calculate metric S4, comparing date and quantity buy on each row.\n",
    "\n",
    "        :returns: the score on this metric (between 0 and 1)\n",
    "\n",
    "        \"\"\"\n",
    "        date_col = self._gt_t_col_it['date']\n",
    "        id_item_col = self._gt_t_col_it['id_item']\n",
    "\n",
    "        f_hat = self._evaluate([date_col, id_item_col])\n",
    "\n",
    "        score = self.compare_f_files(f_hat)\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _s5_metric(self):\n",
    "        \"\"\"Calculate metric S5, comparing date and quantity buy on each row.\n",
    "\n",
    "        :returns: the score on this metric (between 0 and 1)\n",
    "\n",
    "        \"\"\"\n",
    "        id_item_col = self._gt_t_col_it['id_item']\n",
    "        price_col = self._gt_t_col_it['price']\n",
    "        qty_col = self._gt_t_col_it['qty']\n",
    "\n",
    "        f_hat = self._evaluate([id_item_col, price_col, qty_col])\n",
    "\n",
    "        score = self.compare_f_files(f_hat)\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _s6_metric(self):\n",
    "        \"\"\"Calculate metric S6, comparing date and quantity buy on each row.\n",
    "\n",
    "        :returns: the score on this metric (between 0 and 1)\n",
    "\n",
    "        \"\"\"\n",
    "        id_item_col = self._gt_t_col_it['id_item']\n",
    "        date_col = self._gt_t_col_it['date']\n",
    "        price_col = self._gt_t_col_it['price']\n",
    "\n",
    "        f_hat = self._evaluate([id_item_col, date_col, price_col])\n",
    "\n",
    "        score = self.compare_f_files(f_hat)\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _s7_metric(self):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        def _reid_multi(month):\n",
    "            return self._find_k_guess(*self._subset(month))\n",
    "\n",
    "        Df_list = list()\n",
    "        F_list = list()\n",
    "        with PPool(SIZE_POOL) as p:\n",
    "            F_list = p.map( _reid_multi, [i for i in range(13)])\n",
    "        F_list = sorted(F_list, key=lambda x : x['month'], reverse = False)\n",
    "        dtypes = {'id_user': str}\n",
    "        user_id = pd.DataFrame(sorted(self._ground_truth[\"id_user\"].unique()))\n",
    "        user_id.columns = ['id_user']\n",
    "        user_id = user_id.set_index('id_user')\n",
    "        Df_list.append(user_id)\n",
    "        for dict_month in F_list:\n",
    "            col = dict_month.pop(\"month\")\n",
    "            df = pd.DataFrame.from_dict(dict_month, orient='index')\n",
    "            df.columns = [col]\n",
    "            Df_list.append(df)\n",
    "        F_file = pd.concat(Df_list, axis=1, join_axes=[Df_list[0].index])\n",
    "        F_file = F_file.reset_index()\n",
    "        F_file = F_file.fillna('DEL')\n",
    "\n",
    "        score = self.compare_f_files(F_file)\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _calc_sim_mat_dist(self, item_item_dic1, item_item_dic2):\n",
    "        \"\"\" Calcul the distance between two item_item matrix.\n",
    "\n",
    "        return: the distance between the matrix, max value is 1\n",
    "        \"\"\"\n",
    "        sim_dist = 0\n",
    "        item_item_dic1_sum = 0\n",
    "\n",
    "        for item_no, item2_no in item_item_dic1:\n",
    "            item_item_dic1_sum += item_item_dic1[(item_no, item2_no)]\n",
    "\n",
    "            if (item_no, item2_no) in item_item_dic2:\n",
    "                sim_dist += math.fabs(item_item_dic1[(item_no, item2_no)] - item_item_dic2[(item_no, item2_no)])\n",
    "            else:\n",
    "                sim_dist += item_item_dic1[(item_no, item2_no)]\n",
    "\n",
    "        sim_dist /= item_item_dic1_sum\n",
    "\n",
    "        if sim_dist > 1.0:\n",
    "            sim_dist = 1.0\n",
    "        return sim_dist\n",
    "\n",
    "    def _compare_date_gt_anon(self):\n",
    "        \"\"\"Compare each date in T and AT, it's mean to value the transformation applied to the date\n",
    "        in AT.\n",
    "\n",
    "        :returns: The total of all the difference between the date in T and AT\n",
    "\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "\n",
    "        for idx in range(self._anon_trans.shape[0]):\n",
    "            # Skip this loop if id_user == DEL\n",
    "            if self._anon_trans.iloc[idx, self._gt_t_col_it['id_user']-1] == \"DEL\":\n",
    "                continue\n",
    "\n",
    "            # Get the date from the data at index idx\n",
    "            #  TODO: .iloc should be used here for safety reason but iloc does not keep columns\n",
    "            #  value <12-06-18, Antoine> #\n",
    "            gt_day = self._ground_truth.loc[idx, self._gt_t_col['date']]\n",
    "            anon_day = self._anon_trans.loc[idx, self._gt_t_col['date']]\n",
    "\n",
    "            #  TODO: Mettre ça dans un fichier qui test tous les formats et fait des messages\n",
    "            #  d'erreurs approprié  <12-06-18, Antoine> #\n",
    "            gt_day = pd.datetime.strptime(gt_day, '%Y/%m/%d')\n",
    "            anon_day = pd.datetime.strptime(anon_day, '%Y/%m/%d')\n",
    "\n",
    "            score += abs((gt_day - anon_day).days)\n",
    "\n",
    "        score = np.round(float(score)/float(31 * self._anon_trans.shape[0]), 10)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compare_price_gt_anon(self):\n",
    "        \"\"\"Compare each price in T and AT, it's mean to value the transformation applied to the\n",
    "        price in AT.\n",
    "\n",
    "        :returns: The total of all the difference between the price in T and AT\n",
    "\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "\n",
    "        for idx in range(self._anon_trans.shape[0]):\n",
    "            # Skip this loop if id_user == DEL\n",
    "            if self._anon_trans.iloc[idx, self._gt_t_col_it['id_user']-1] == \"DEL\":\n",
    "                continue\n",
    "\n",
    "            # Get the date from the data at index idx\n",
    "            gt_price = float(self._ground_truth.loc[idx, self._gt_t_col['price']])\n",
    "            anon_price = float(self._anon_trans.loc[idx, self._gt_t_col['price']])\n",
    "\n",
    "            # Get the difference between date1 and date2 in days\n",
    "            score += (1 - min(gt_price, anon_price)/max(gt_price, anon_price))\n",
    "\n",
    "        score = np.round(float(score)/float(self._anon_trans.shape[0]), 10)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_median_qty(self):\n",
    "        \"\"\"Compute the median of all qty of item buyed\n",
    "\n",
    "        :return: median of all qty of item buyed\n",
    "        \"\"\"\n",
    "        # Creating item x user sparse matrix for the ground_truth\n",
    "        gt_id_item_id_user = self.ground_truth.groupby(\n",
    "            [self._gt_t_col['id_item'], self._gt_t_col['id_user']]\n",
    "        )[self._gt_t_col['qty']].sum()\n",
    "\n",
    "        # Recovering median\n",
    "        median = gt_id_item_id_user.median()\n",
    "\n",
    "        return median\n",
    "\n",
    "    def _collaborative_filtering_item_user(self, data, e2=False):\n",
    "        \"\"\"Compute the matrix of cosine similarity between all items\n",
    "\n",
    "        :data: dataframe from which to compute the collaborative_filtering\n",
    "        :returns: matrix |col_item|x|col_item|\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        id_items_ori = self.ground_truth[self.gt_t_col[\"id_item\"]].unique()\n",
    "\n",
    "        # Creating item x user sparse matrix for the ground_truth\n",
    "        data_id_item_id_user = data.groupby(\n",
    "            [self._gt_t_col['id_item'], self._gt_t_col['id_user']]\n",
    "        )[self._gt_t_col['qty']].sum()\n",
    "\n",
    "        if e2:\n",
    "            # item x user < median\n",
    "            data_id_item_id_user = data_id_item_id_user[\n",
    "                data_id_item_id_user < self._compute_median_qty()\n",
    "            ]\n",
    "\n",
    "        # Create dataframe matrix\n",
    "        data_id_item_id_user = data_id_item_id_user.unstack(level=1).to_sparse()\n",
    "\n",
    "        items_differ = set(id_items_ori) - set(data_id_item_id_user.index)\n",
    "\n",
    "        if items_differ:\n",
    "            df_temp = pd.DataFrame(index=items_differ, columns=data_id_item_id_user.columns)\n",
    "            data_id_item_id_user = data_id_item_id_user.append(df_temp)\n",
    "\n",
    "        data_id_item_id_user = data_id_item_id_user.sort_index()\n",
    "\n",
    "        return cosine_similarity(data_id_item_id_user.fillna(0))\n",
    "\n",
    "    def _e1_metric(self):\n",
    "        \"\"\" Construct a similarity matrix of item buyed (User that have bought this item also bought\n",
    "        item_i). Here the score is maximized if the quantity is high (calculated by dozen). We\n",
    "        calculate the difference between the two matrix of item buyed as a score.\n",
    "\n",
    "        More precisly we construct two matrix M1 and M2, one for the original dataset and one for\n",
    "        the anonymised one. Both are of size `n x n` where `n` is the number of item. For M_ij\n",
    "        represent the number of people who have bought the item i and have also bought the item j.\n",
    "\n",
    "        This procede is called a collaborative filtering\n",
    "        (https://en.wikipedia.org/wiki/Collaborative_filtering).\n",
    "\n",
    "        :returns: score of the metric.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Copy dataframe\n",
    "        ground_truth = self._ground_truth.copy()\n",
    "        anon_trans = self._anon_trans.copy()\n",
    "        anon_trans = anon_trans.drop(\n",
    "            anon_trans[anon_trans[self._gt_t_col['id_user']] == 'DEL'].index, axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        #  TODO: can be done one time only  <21-12-18, yourname> #\n",
    "        anon_trans[self._gt_t_col['qty']] = anon_trans[self._gt_t_col['qty']].apply(int)\n",
    "\n",
    "        # Compute the cosinus similarity item x item\n",
    "        gt_cos_sim = self._collaborative_filtering_item_user(ground_truth)\n",
    "\n",
    "        # Compute the cosinus similarity item x item\n",
    "        at_cos_sim = self._collaborative_filtering_item_user(anon_trans)\n",
    "\n",
    "        # Compute score\n",
    "        diff_cos = abs(gt_cos_sim - at_cos_sim)\n",
    "        score = min(1, diff_cos.sum() / gt_cos_sim.sum())\n",
    "\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _e2_metric(self):\n",
    "        \"\"\" Construct a similarity matrix of item buyed (User that have bought this item also bought\n",
    "        item_i). Here the score is maximized if the quantity is low (<= threshold). We\n",
    "        calculate the difference between the two matrix of item buyed as a score.\n",
    "\n",
    "        More precisly we construct two matrix M1 and M2, one for the original dataset and one for\n",
    "        the anonymised one. Both are of size `n x n` where `n` is the number of item. For M_ij\n",
    "        represent the number of people who have bought the item i and have also bought the item j.\n",
    "\n",
    "        This procede is called a collaborative filtering\n",
    "        (https://en.wikipedia.org/wiki/Collaborative_filtering).\n",
    "\n",
    "        :returns: score of the metric.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if True:\n",
    "            return 0\n",
    "\n",
    "        # Copy dataframe\n",
    "        ground_truth = self._ground_truth.copy()\n",
    "        anon_trans = self._anon_trans.copy()\n",
    "        anon_trans = anon_trans.drop(\n",
    "            anon_trans[anon_trans[self._gt_t_col['id_user']] == 'DEL'].index, axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        #  TODO: can be done one time only  <21-12-18, yourname> #\n",
    "        anon_trans[self._gt_t_col['qty']] = anon_trans[self._gt_t_col['qty']].apply(int)\n",
    "\n",
    "        # Compute the cosinus similarity item x item\n",
    "        gt_cos_sim = self._collaborative_filtering_item_user(ground_truth, e2=True)\n",
    "\n",
    "        # Compute the cosinus similarity item x item\n",
    "        at_cos_sim = self._collaborative_filtering_item_user(anon_trans, e2=True)\n",
    "\n",
    "        # Compute score\n",
    "        diff_cos = abs(gt_cos_sim - at_cos_sim)\n",
    "        score = min(1, diff_cos.sum() / gt_cos_sim.sum())\n",
    "\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _e3_metric(self):\n",
    "        \"\"\" Caluclate the difference (as in set difference) and similarity matrix between top-`k`\n",
    "        items bought from ground truth and anonymised dataset.\n",
    "\n",
    "        :returns: score of the metric.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Copy dataframe\n",
    "        ground_truth = self._ground_truth.copy()\n",
    "        anon_trans = self._anon_trans.copy()\n",
    "        anon_trans = anon_trans.drop(\n",
    "            anon_trans[anon_trans[self._gt_t_col['id_user']] == 'DEL'].index, axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        #  TODO: can be done one time only  <21-12-18, yourname> #\n",
    "        anon_trans[self._gt_t_col['qty']] = anon_trans[self._gt_t_col['qty']].apply(int)\n",
    "\n",
    "        # Computing top 5% of most purchased item by customer\n",
    "        item_count = ground_truth.groupby(self._gt_t_col['id_item']).size()\n",
    "        gt_top_k = list(\n",
    "            item_count.sort_values(ascending=False).head(int(item_count.shape[0]*0.05)).index\n",
    "        )\n",
    "\n",
    "        # Ground Truth is now with top k items\n",
    "        ground_truth = ground_truth.set_index(self._gt_t_col['id_item'])\n",
    "        ground_truth = ground_truth.loc[gt_top_k]\n",
    "        ground_truth = ground_truth.reset_index()\n",
    "\n",
    "        # Compute the cosinus similarity item x item\n",
    "        gt_cos_sim = self._collaborative_filtering_item_user(ground_truth)\n",
    "\n",
    "        # Computing top 5% of most purchased item by customer\n",
    "        item_count = anon_trans.groupby(self._gt_t_col['id_item']).size()\n",
    "        at_top_k = list(\n",
    "            item_count.sort_values(ascending=False).head(int(item_count.shape[0]*0.05)).index\n",
    "        )\n",
    "\n",
    "        # Anonymized File is now with top k items\n",
    "        anon_trans = anon_trans.set_index(self._gt_t_col['id_item'])\n",
    "        anon_trans = anon_trans.loc[at_top_k]\n",
    "        anon_trans = anon_trans.reset_index()\n",
    "\n",
    "        # Compute the cosinus similarity item x item\n",
    "        at_cos_sim = self._collaborative_filtering_item_user(anon_trans)\n",
    "\n",
    "        # Compute score\n",
    "        diff_cos = min(1, (abs(gt_cos_sim - at_cos_sim).sum()) / gt_cos_sim.sum())\n",
    "        # Jaccard distance\n",
    "        diff_top_k = (\n",
    "            len(set(gt_top_k).union(at_top_k)) - len(set(gt_top_k).intersection(set(at_top_k)))\n",
    "        )/ len(set(gt_top_k).union(at_top_k))\n",
    "\n",
    "        score = max(diff_cos, diff_top_k)\n",
    "\n",
    "        # Add the score to the global score for this metric\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _e4_metric(self):\n",
    "        \"\"\" Calculate the mean distance in day between anonymised and ground truth\n",
    "        transactions.\n",
    "\n",
    "        :returns: score of the metric.\n",
    "\n",
    "        \"\"\"\n",
    "        score = self._compare_date_gt_anon()\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _e5_metric(self):\n",
    "        \"\"\" Calculate the difference, as the ratio, of all item prices.\n",
    "\n",
    "        :returns: score of the metric.\n",
    "\n",
    "        \"\"\"\n",
    "        score = self._compare_price_gt_anon()\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _e6_metric(self):\n",
    "        \"\"\" Calculate the ratio between the number of lines removed in the anonymized table over the\n",
    "        number of lines in the original dataset.\n",
    "\n",
    "        :returns: score of the metric.\n",
    "\n",
    "        \"\"\"\n",
    "        score = np.round(float(1 - self._anonymized.shape[0]/self._ground_truth.shape[0]), 4)\n",
    "        self._current_score.append(score)\n",
    "\n",
    "        return score\n",
    "\n",
    "    @property\n",
    "    def f(self):\n",
    "        \"\"\"\n",
    "        Get the original file F.\n",
    "        \"\"\"\n",
    "        return self._f_orig\n",
    "\n",
    "\n",
    "    @property\n",
    "    def users(self):\n",
    "        \"\"\"\n",
    "        Get the users data\n",
    "        \"\"\"\n",
    "        return self._users\n",
    "\n",
    "    @property\n",
    "    def ground_truth(self):\n",
    "        \"\"\"\n",
    "        Get the ground_truth data\n",
    "        \"\"\"\n",
    "        return self._ground_truth\n",
    "\n",
    "    @property\n",
    "    def anonymized(self):\n",
    "        \"\"\"\n",
    "        Get the anonymized data\n",
    "        \"\"\"\n",
    "        return self._anonymized\n",
    "\n",
    "    @property\n",
    "    def users_t_col(self):\n",
    "        \"\"\"\n",
    "        Get the column used in the M csv\n",
    "        \"\"\"\n",
    "        return self._users_t_col\n",
    "\n",
    "    @property\n",
    "    def gt_t_col(self):\n",
    "        \"\"\"\n",
    "        Get the column used in the T csv\n",
    "        \"\"\"\n",
    "        return self._gt_t_col\n",
    "\n",
    "    @property\n",
    "    def current_score(self):\n",
    "        \"\"\"\n",
    "        Get the current score calculated by the metrics\n",
    "        \"\"\"\n",
    "        return self._current_score\n",
    "\n",
    "def metric_wrapper(metric, instance, numero):\n",
    "    \"\"\"Launch a metric in function of instance metric and the number of the later.\n",
    "\n",
    "    :metric: a single char wich is 's' or 'e', respectivly for reid and utility metrics.\n",
    "    :instance: the instance of a Metric class containing methods `metric`.\n",
    "    :numero: the ieme method of the instance you want to call.\n",
    "\n",
    "    :returns: Result of the metric method called.\n",
    "\n",
    "    \"\"\"\n",
    "    method = \"_{}{}_metric\".format(metric, numero)\n",
    "    return getattr(instance, method)()\n",
    "\n",
    "def utility_metric(ground_truth, sub):\n",
    "    \"\"\"TODO: Docstring for utility_metric.\n",
    "\n",
    "    :arg1: TODO\n",
    "    :returns: TODO\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize utility metrics\n",
    "    metric = Metrics(ground_truth, sub)\n",
    "\n",
    "    #Compute utility metrics as subprocesses\n",
    "    metric_pool = Pool()\n",
    "    utility_wrapper = partial(metric_wrapper, \"e\", metric)\n",
    "    utility_scores = metric_pool.map(utility_wrapper, range(1, 7))\n",
    "\n",
    "    #Compute reidentification metrics as subprocesses\n",
    "    #metric_pool = Pool()\n",
    "    #reid_wrapper = partial(metric_wrapper, \"s\", metric)\n",
    "    #reid_scores = metric_pool.map(reid_wrapper, range(1, 8))\n",
    "\n",
    "    return utility_scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
