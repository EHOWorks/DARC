{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/ground_truth.csv' does not exist: b'data/ground_truth.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f3fe56af80f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/ground_truth.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Need to find a way to remove the additional date 1900-01-01 while keeping datetime format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hours\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hours\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%H:%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/ground_truth.csv' does not exist: b'data/ground_truth.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ground_truth.csv\", parse_dates=[\"date\"])\n",
    "# Need to find a way to remove the additional date 1900-01-01 while keeping datetime format. \n",
    "df[\"hours\"]= pd.to_datetime(df[\"hours\"], format = '%H:%M%S')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id_item\"]=df[\"id_item\"].astype(\"category\")\n",
    "df[\"date\"]=df[\"date\"].astype(\"category\")\n",
    "categorical = set ({'date','id_item'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial MU : 14.1+ MB\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quasi identification sur base de date et d'heure\n",
    "df[(df[\"date\"]==\"2010-12-01\") & (df[\"hours\"]==\"1900-01-01 08:02:06\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique users\n",
    "\n",
    "df[\"id_user\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The quantity bought by user 17850 is very often 6 (215/297=72%)\n",
    "\n",
    "#print(df[df[\"id_user\"]==12680].head(1000).to_string())\n",
    "df[df[\"id_user\"]==17850][\"qty\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The item 84406B is most bought by user 17850 (15/176 = 8% vs 0.9% mean per other user so more than x8 times )\n",
    "\n",
    "df[df[\"id_item\"]==\"84406B\"][\"id_user\"].value_counts()\n",
    "#df[df[\"id_item\"]==\"84406B\"][\"id_user\"].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user 12688 is the one who most bought at the date 2011-08-18\n",
    "\n",
    "df[df[\"date\"]==\"2011-08-18\"][\"id_user\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{df[\"qty\"].min(),df[\"qty\"].max()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of k-anonymity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(df,partition, scale = None):\n",
    "    spans={}\n",
    "    for column in df.columns:\n",
    "        if column in categorical:\n",
    "            span=len(df[column][partition].unique())\n",
    "        else:\n",
    "            span=df[column][partition].max()-df[column][partition].min()\n",
    "        if scale is not None:\n",
    "            span = span/scale[column]\n",
    "        spans[column]=span\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_spans = get_spans(df,df.index)\n",
    "full_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, partition, column):\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    :param     df: The dataframe from which we want to split a partition\n",
    "    :param     partition: The range of indexes (rows) we want to select from df \n",
    "    :param     column: The column to select from df, and operate the split with\n",
    "    \n",
    "    Example : \n",
    "    \n",
    "    index         age(numerical)   gender(categorical)\n",
    "      0            19              female\n",
    "      1            25              female\n",
    "      2            14              male\n",
    "      3            51              female\n",
    "\n",
    "      \n",
    "      If column = age \n",
    "      Median = (S[2]+S[1])/2 = (14+25)/2 = 19.5 ; Sorted age : {14,19,25,51} = {S[2],S[0],S[1],S[3]}\n",
    "      return (Range{2,0},Range{1,3})\n",
    "      \n",
    "      If column=gender (It is precisely because of even cardinals that we can't calculate medians for non numerical)\n",
    "      values = {gender,female}\n",
    "      return (Range{2},Range{0,1,3}) \n",
    "      \n",
    "      As shown above, cases where the partition is split on a non numerical column, can result in a weird situation\n",
    "      where left and right partition don't really have the same number of values.\n",
    "      And this is only a showcase situation, it is virtually possible to have 0 value at left, and all values at right\n",
    "      (which also happens when the quasi identifier has only one value in the partition : card(values)=1)\n",
    "      or 75% at left and 25% at right etc... \n",
    "      \n",
    "      So really for categorical values, the notion of median (which is supposed to split the set in 50%) doesn't make\n",
    "      any sens here.\n",
    "\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                                            dfp = df[column][partition]\n",
    "                                            \n",
    "                          df[column] is a set (like an array one dimension, index : value)\n",
    "                          df[column][partition] is a subset of df[column] (selection of rows)\n",
    "                          It is also treated as a set\n",
    "                          \n",
    "                            Example: \n",
    "                            \n",
    "                            df[age] = 0 19 = df[df.index]\n",
    "                                      1 25\n",
    "                                      2 14\n",
    "                                      3 51\n",
    "                            \n",
    "                          df[column][0] = 19 \n",
    "                          df[column][[0,1]]= 19\n",
    "                                             25\n",
    "                          [0,1] can be viewed as the range of indexes from 0 to 1 included\n",
    "                          df.index is the range of all indexes from 0 to df.len-1\n",
    "                          partition is nothing more than the set of indexes of a given number of rows\n",
    "                            \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "   \n",
    "     \n",
    "    \n",
    "    dfp = df[column][partition]\n",
    "    \n",
    "    if ((column in categorical) | (column == \"date\") | (column == \"hours\")):\n",
    "        values = dfp.unique()\n",
    "        lv=set(values[:len(values)//2])\n",
    "        rv=set(values[len(values)//2:])\n",
    "        return dfp.index[dfp.isin(lv)],dfp.index[dfp.isin(rv)]\n",
    "    else:\n",
    "        median=dfp.median()\n",
    "        #print(dfp, median)\n",
    "        dfl=dfp.index[dfp<median]\n",
    "        dfr=dfp.index[dfp>=median]\n",
    "        #print(df[\"qty\"][dfl])\n",
    "        return(dfl,dfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "39/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl,dfr = split(df,df.index,\"id_item\")\n",
    "dfl,dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If partition has less members than k then it's not a valid one, so we can't further divide it\n",
    "def is_k_anonymous (df, partition, sensitive_column, k=5000):\n",
    "    if len(partition)<k:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(df, feature_columns,sensitive_column, scale, is_valid):  \n",
    "    \"\"\"\"\"\n",
    "    :param     df: The dataframe to partition\n",
    "    :param     feature_columns: The Quasi-Identifier columns \n",
    "    :param     sensitive_column : The Sensitive Data we wish to protect\n",
    "    :param     scale : original full_spans of df before the first split\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    finished_partitions=[]\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                                            [df.index]\n",
    "                                            \n",
    "                    df.index is a range of indexes, it can be seen as an array of indexes while it's not \n",
    "                    technically an array.\n",
    "                    [df.index] in between brackets simply means you're initiliazing a list of index ranges, \n",
    "                    you can see it as an array of arrays.\n",
    "                            \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    partitions=[df.index]\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                                            while partitions:\n",
    "                                            \n",
    "                    The main loop here works like a chef, think of partitions like a cucumber, \n",
    "                    each time we're going through the while, chef cuts it in two equal parts (more or less **)\n",
    "                                    (i1) <----> => (i2) <--><--> =>  (i3) <-><-><-><->\n",
    "                                        PS: Sorry for the poor drawing skills\n",
    "                        ** Refer to the split method to understand this remarque.\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    while partitions:\n",
    "        \n",
    "        # Pop the oldest partition from the list of partitions, so that we can split it further (or at least try).\n",
    "        # Remember partition is the same type as df.index so it's a range of indexes (an \"array\" of indexes)\n",
    "        \n",
    "        partition = partitions.pop(0)\n",
    "        \n",
    "        # Update the spans (number of unique values for each quasi-identifier column) for this iteration\n",
    "        \n",
    "        spans = get_spans(df[feature_columns],partition, scale)\n",
    "        \n",
    "        \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                            for column, span in sorted(spans.items(), key = lambda x:-x[1]):\n",
    "                    \n",
    "                    for {column,span} in {column1 : numberOfUniqueItemInColumn_1    \n",
    "                                          column2 : numberOfUniqueItemInColumn_2   \n",
    "                                          ....\n",
    "                                          columnN : numberOfUniqueItemInColumn_N   \n",
    "                                          } \n",
    "                    where numberOfUniqueItemInColumn_i is a drecreasing sequence (suite décroissante) \n",
    "                    \n",
    "                    Here we loop through the quasi identifiers (since span stores the quasi identifier column\n",
    "                    names and values) ordered in decreasing order of multiplicity (number of # values).\n",
    "                    \n",
    "                    \n",
    "                    Details :\n",
    "                    \n",
    "                    spans.items() returns a hashmap of spans column names as keys, and span column values as values\n",
    "                    \n",
    "                    key = lambda x:-x[1] tells to sorted(), hey I want you to sort this hashmap in decreasing order \n",
    "                    of the elements that are in [1] so in decreasing values in this case. \n",
    "                    (key = labmda x:x[0] would've returned a sorted hashmap in increasing order of indexes which \n",
    "                    is the default)\n",
    "             \n",
    "                    \n",
    "        \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "        for column, span in sorted(spans.items(), key = lambda x:-x[1]):\n",
    "            \n",
    "            # lp and rp are like partition : they are a range of indexes (an \"array\" of indexes)\n",
    "            # lp is the ranges of indexes for whome df[column][lp] < median\n",
    "            # rp is the ranges of indexes for whome df[column][rp] >= median\n",
    "\n",
    "            lp, rp = split(df,partition,column)\n",
    "            \n",
    "            \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    if not is_valid(df,lp,sensitive_column) or not is_valid(df,rp,sensitive_column):\n",
    "                                            \n",
    "                    As long as one of both split partitions is still valid (len(partition)>k) we need\n",
    "                    to break from the for loop (don't even look for next quasi identifier) and add lp and rp \n",
    "                    to the list of partitions we want to cut even more.\n",
    "                    \n",
    "                    Otherwise we enter the if, and execute continue : \n",
    "                    What continue does, is forget the rest of the loop and skip to the next iteration,\n",
    "                    so select the next quasi identifier to divide the partition.\n",
    "                    \n",
    "                    If we tried with all quasi identifiers, and none worked, then our partition is done (as\n",
    "                    small as possible) and when we execute continue it will have no more quasi to explore. \n",
    "                    So executes the else and append the finished partition to the set of finished_partitions.\n",
    "        \n",
    "                            \n",
    "            \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            if not is_valid(df,lp,sensitive_column) or not is_valid(df,rp,sensitive_column):\n",
    "                continue\n",
    "            partitions.extend((lp,rp))\n",
    "            break\n",
    "        else:\n",
    "            finished_partitions.append(partition)\n",
    "    return finished_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns=[\"date\", \"qty\"]\n",
    "sensitive_column=\"price\"\n",
    "finished_partitions=partition_dataset(df,feature_columns,sensitive_column, full_spans, is_k_anonymous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finished_partitions)\n",
    "#df[\"date\"][finished_partitions[0]]\n",
    "#finished_partitions[0]\n",
    "#finished_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)/305\n",
    "df[\"qty\"][finished_partitions[5]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various handlers for .agg() method to call on each column of df.loc[partition] depending on the column type\n",
    "\n",
    ":param   series: Column Set (from df.loc[partition]) to be processed \n",
    "\"\"\"\n",
    "\n",
    "def agg_categorical_column(series):\n",
    "    return[','.join(set(series))]\n",
    "\n",
    "def agg_numerical_column(series):\n",
    "    return[series.mean()]\n",
    "\n",
    "def agg_date_column(series):\n",
    "    # remove the hh:mm:ss\n",
    "    #series=series.dt.date\n",
    "    if(isinstance(series,pd.Series)):\n",
    "        print(series[series.idxmin], series[series.idxmax])\n",
    "        return[[series[series.idxmin], series[series.idxmax]]]\n",
    "    #return [[series.max(),series.min()]]\n",
    "    return [\"something\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_anonymized_dataset(df,partitions,feature_columns,sensitive_column,max_partitions=None):\n",
    "    aggregations = {}\n",
    "    for column in feature_columns:\n",
    "        if column in categorical:\n",
    "            if column == \"date\":\n",
    "                aggregations[column]=agg_date_column\n",
    "            else:\n",
    "                aggregations[column]=agg_categorical_column\n",
    "            \n",
    "        else: \n",
    "            aggregations[column]=agg_numerical_column\n",
    "            \n",
    "    rows=[]\n",
    "    for i, partition in enumerate(partitions):\n",
    "        if i%100==1:\n",
    "            print(\"Finished {} partitions ! \".format(i) )\n",
    "        if max_partitions is not None and i > max_partitions:\n",
    "            break\n",
    "        #df.agg({column : method_to_apply})\n",
    "        #df.agg({numerical_column : agg_numerical_column\n",
    "        #         categorical_column : aggww_categorical_column\n",
    "        #        })\n",
    "        # Result 1 line multiple column, each cell=aggreg result\n",
    "        \n",
    "        #print(aggregations)\n",
    "        #print(df.loc[partition].agg(aggregations,squeeze=False))\n",
    "        #print(type(df.loc[partition][\"date\"]))\n",
    "        grouped_columns=df.loc[partition].agg(aggregations,squeeze=False)\n",
    "        # Count spans of sensitive column in a partition\n",
    "        sensitive_counts = df.loc[partition].groupby(sensitive_column).agg({\n",
    "            sensitive_column : 'count'\n",
    "        })\n",
    "        values = grouped_columns.iloc[0].to_dict()\n",
    "        #print(values)\n",
    "        for sensitive_value, count in sensitive_counts[sensitive_column].items():\n",
    "            if count==0:\n",
    "                continue\n",
    "            values.update({\n",
    "                sensitive_column : sensitive_value,\n",
    "                'count' : count,\n",
    "            })\n",
    "            rows.append(values.copy())\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn=build_anonymized_dataset(df,finished_partitions,feature_columns,sensitive_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors=dfn.groupby(\"date\")\n",
    "#dfn[\"date\"].value_counts().head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.to_csv(\"k_anon:date+qty:price.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"date\"].min()-df[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"]=pd.DatetimeIndex(df[\"date\"]).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"date\"]==\"2010-12-01\"][\"id_user\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fea6c5cfb03a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id_item\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"id_item\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
